{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0428e8f7",
   "metadata": {},
   "source": [
    "# What is a Decision Tree\n",
    "\n",
    "According to <a href=\"https://en.wikipedia.org/wiki/Decision_tree\">Wikipedia.</a>\n",
    "<p><i><h4>\n",
    "\"A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements\"\n",
    "    </h4>   </i></p>\n",
    "\n",
    "<h2>What does this mean? </h2>\n",
    "\n",
    "The idea is that you take decision on each node and eventually you know what decision you should take. Like, If the <b>outlook</b> is <b><i>sunny</i></b> and <b>humidity</b> is <b><i>low</i></b> . I think I will say <b>yes</b> to playing tennis and it should look something like this\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/Decision_Tree-2.png\" alt=\"Decision Tree\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca472e",
   "metadata": {},
   "source": [
    "# How to Construct a Decision Tree\n",
    "\n",
    "<p>\n",
    "That is where the problem starts ? \n",
    "\n",
    "Almost everyone after looking at decision tree can tell how they work but when you raise the question how to build it programatically most of the people will look away. \n",
    "\n",
    "But why is that, The structure looks so simple and easy to grasp, it should be easy to implement as well. \n",
    "\n",
    "Answer is \"yes\" it is easy to implement, most of the people have not thought how to implement it programmatically because the visual interpretation is so close to how we make our decisions , it becomes almost intuitive. And once you have intuition about something you will not have too many questions. \n",
    "\n",
    "For example if we show Neural netwrok to people and tell them this works with 90% accuracy, most of the people would say <b> \"how?\" </b>, because the neural network structure is not intuitive at first glance.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b5774",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "We will be using an example dataset throughout (Famous Iris Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ae1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2122b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf43670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3846977",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72cce6a",
   "metadata": {},
   "source": [
    "# 2. Extract Rules from Decision Tree with Scikit-Learn and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eedc84",
   "metadata": {},
   "source": [
    "The rules extraction from the Decision Tree can help with better understanding how samples propagate through the tree during the prediction. It can be needed if we want to implement a Decision Tree without Scikit-learn or different than Python language. Decision Trees are easy to move to any programming language because there are set of if-else statements. I’ve seen many examples of moving scikit-learn Decision Trees into C, C++, Java, or even SQL.\n",
    "\n",
    "In this lecture, I will show you how to get decision rules from the Decision Tree (for both classification and regression tasks) with following approaches:\n",
    "\n",
    "built-in text representation,\n",
    "convert a Decision Tree to the code (can be in any programming language)\n",
    "convert a Decision Tree to set of rules which are human-readable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7f028",
   "metadata": {},
   "source": [
    "## 2.1 Train Decision Tree on Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68039bc3",
   "metadata": {},
   "source": [
    "Let's train a DecisionTreeClassifier on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb9dce",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "The **DecisionTreeClassifier** is a classification model from the Scikit-learn library based on the concept of a decision tree. The DecisionTreeClassifier builds a decision tree from the given training data and then uses it to classify new input data. This model is particularly useful in identifying patterns and relationships in complex, multidimensional data sets. It is easy to understand and interpret, and it is also robust to changes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178de3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier with default hyper-parameters\n",
    "clf = DecisionTreeClassifier(random_state=1234)\n",
    "model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a68abd",
   "metadata": {},
   "source": [
    "### Scikit-Learn Built-in Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1165a",
   "metadata": {},
   "source": [
    "The Scikit-Learn Decision Tree class has an export_text(). It returns the text representation of the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eadaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text representation\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba00b37",
   "metadata": {},
   "source": [
    "### You can pass the feature names as the argument to get better text representation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33950205",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = tree.export_text(clf, feature_names=iris.feature_names,class_names=list(iris.target_names))\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb079c",
   "metadata": {},
   "source": [
    "### If you want to save it to the file, it can be done with following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"decistion_tree.log\", \"w\") as fout:\n",
    "    fout.write(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b804e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa956cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1277c",
   "metadata": {},
   "source": [
    "### Plot Tree with plot_tree\n",
    "The plot_tree method was added to sklearn in version 0.21. It requires matplotlib to be installed. It allows us to easily produce figure of the tree (without intermediate exporting to graphviz) The more information about plot_tree arguments are in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f369c9",
   "metadata": {},
   "source": [
    "### The decision tree learning algorithm recursively learns the tree as follows:\n",
    "\n",
    "1. Assign all training instances to the root of the tree. Set curent node to root node.\n",
    "2. For each attribute Partition all data instances at the node by the value of the attribute.\n",
    "3. Compute the information gain ratio from the partitioning.\n",
    "4. Identify feature that results in the greatest information gain ratio. Set this feature to be the splitting criterion at the current node.\n",
    "5. If the best information gain ratio is 0, tag the current node as a leaf and return.\n",
    "6. Partition all instances according to attribute value of the best feature.\n",
    "7. Denote each partition as a child node of the current node.\n",
    "8. For each child node:\n",
    "    a. If the child node is “pure” (has instances from only one class) tag it as a leaf and return.\n",
    "    b. If not set the child node as the current node and recurse to step 2.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa783b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(clf, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=list(iris.target_names),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a7a48",
   "metadata": {},
   "source": [
    "**Gini impurity** is a measure used in decision tree algorithms to quantify a dataset’s impurity level or disorder. In binary classification problems, it assesses the likelihood of an incorrect classification when a randomly selected data point is assigned a class label based on the distribution of classes in a particular node. It ranges from 0 to 0.5, where 0 indicates a perfectly pure node (all instances belong to the same class), and 0.5 signifies maximum impurity (an equal distribution of classes). In decision trees, it aids in selecting the optimal split by identifying features that result in more homogeneous subsets of data, ultimately contributing to the creation of accurate and reliable predictive models.\n",
    "\n",
    "## $$ Gini = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "where pi is the proportion of items labeled with class i in the set, and the sum is taken over all unique classes in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b33ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b0dbf55",
   "metadata": {},
   "source": [
    "## 2.2 DecisionTreeRegressor\n",
    "**DecisionTreeRegressor** is a class provided by scikit-learn, a popular machine learning library in Python, used to perform regression tasks with decision trees. Similar to DecisionTreeClassifier, which is used for classification tasks, DecisionTreeRegressor predicts the value of a target variable by learning simple decision rules inferred from the data features. However, unlike classification that predicts discrete labels, regression is used to predict continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35075b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Fit the regressor, set max_depth = 3\n",
    "regr = #Your code\n",
    "model = #Your code\n",
    "\n",
    "text_representation = tree.export_text(regr)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = #Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b5952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03223402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55a750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1ed258d",
   "metadata": {},
   "source": [
    "# 3. Decision Tree Classification Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = #Your code\n",
    "\n",
    "# Create a Decision Tree Classifier model\n",
    "classifier = # Your code \n",
    "\n",
    "# Train the model\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelnames = list(iris.target_names)\n",
    "targetname = [0,1,2]\n",
    "# For visualization, we'll use only two dimensions: Petal Length and Petal Width\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting the true labels\n",
    "plt.subplot(1, 2, 1)\n",
    "# Your code: \n",
    "\n",
    "\n",
    "# please also add your name on title\n",
    "plt.title(\"<Your name>+True Labels\")\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the predicted labels\n",
    "plt.subplot(1, 2, 2)\n",
    "#Your code:\n",
    "\n",
    "\n",
    "# please also add your name on title\n",
    "plt.title(\"<Your name>+Predicted Labels by Decision Tree\")\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba795207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b324bb-8f63-40ff-8cbc-acbd562b7c16",
   "metadata": {},
   "source": [
    "# Practice Lab: Decision Tree Classifier\n",
    "\n",
    "### Objective\n",
    "In this exercise, you will:\n",
    "1. Train a **Decision Tree** classifier using a small dataset.  \n",
    "2. Visualize the decision tree structure.  \n",
    "3. Interpret how features influence the final decision.  \n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Description\n",
    "You will use a simple dataset of students deciding whether to **pass an exam** based on two factors:\n",
    "\n",
    "| Hours_Studied | Attendance | Pass |\n",
    "|----------------|-------------|------|\n",
    "| 2 | Low | No |\n",
    "| 4 | Low | No |\n",
    "| 6 | Medium | Yes |\n",
    "| 8 | High | Yes |\n",
    "| 5 | High | Yes |\n",
    "| 3 | Medium | No |\n",
    "| 7 | High | Yes |\n",
    "\n",
    "- **Features:**  \n",
    "  - `Hours_Studied` — numeric (continuous)  \n",
    "  - `Attendance` — categorical (`Low`, `Medium`, `High`)  \n",
    "- **Target:** `Pass` (`Yes` / `No`)\n",
    "\n",
    "---\n",
    "## Task Requirements\n",
    "\n",
    "1. Create a small DataFrame using the data above.  \n",
    "2. Encode the categorical variable `Attendance` (e.g., using LabelEncoder).  \n",
    "3. Train a **Decision Tree Classifier** using scikit-learn.  \n",
    "4. Visualize the decision tree using `plot_tree()`.  \n",
    "5. Predict whether a new student who studied **6 hours** and had **Medium attendance** will pass.  \n",
    "6. Explain which feature seems more important to the model.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41874265-9546-4e74-b2c5-e587ad525b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'Hours_Studied': [2, 4, 6, 8, 5, 3, 7],\n",
    "    'Attendance': ['Low', 'Low', 'Medium', 'High', 'High', 'Medium', 'High'],\n",
    "    'Pass': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcfddd-7f3b-4930-8ea5-a1b99868b93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
