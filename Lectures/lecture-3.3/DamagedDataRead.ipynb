{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fedb103",
   "metadata": {},
   "source": [
    "# Read Damaged Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06353432",
   "metadata": {},
   "source": [
    "Data interpretation varies with format, necessitating organization into dataframes for machine readability. This step transforms diverse data into a structured, table-like format, facilitating manipulation and analysis. It's crucial for identifying trends and ensuring data quality, thus setting the foundation for subsequent analytical and predictive tasks in a unified, efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abea2eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us_tourism2damage.csv',\n",
       " 'damaged.json',\n",
       " 'us_tourism1.json',\n",
       " 'us_tourism.csv',\n",
       " 'unstructureddata.txt',\n",
       " 'DamagedDataRead.ipynb',\n",
       " 'us_tourism.json',\n",
       " 'damaged_data.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0605e66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us_tourism2damage.csv', 'us_tourism.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "csvfiles = glob.glob(\"*.csv\")\n",
    "csvfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bc6bd",
   "metadata": {},
   "source": [
    "## 1.1 Read CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4f7891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>State</th>\n",
       "      <th>Attraction</th>\n",
       "      <th>Description</th>\n",
       "      <th>Food Specialty</th>\n",
       "      <th>Food Description</th>\n",
       "      <th>Best Time to Visit</th>\n",
       "      <th>Notable Events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New York</td>\n",
       "      <td>Statue of Liberty</td>\n",
       "      <td>Iconic symbol of freedom</td>\n",
       "      <td>Bagels</td>\n",
       "      <td>Boiled and baked dough rings</td>\n",
       "      <td>Year-round</td>\n",
       "      <td>Fourth of July Celebration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Freedom Trail</td>\n",
       "      <td>Historic 2.5-mile trail</td>\n",
       "      <td>Clam Chowder</td>\n",
       "      <td>Creamy soup with clams and potatoes</td>\n",
       "      <td>Fall</td>\n",
       "      <td>Boston Marathon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Willis Tower</td>\n",
       "      <td>One of the tallest buildings in the US</td>\n",
       "      <td>Deep-Dish Pizza</td>\n",
       "      <td>Thick crust pizza with layers of toppings</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Taste of Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>The Great Lakes</td>\n",
       "      <td>Largest group of freshwater lakes</td>\n",
       "      <td>Pasties</td>\n",
       "      <td>Pastry filled with meat and vegetables</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Great Lakes Shipwreck Festival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Disney World</td>\n",
       "      <td>Famous theme park and resort</td>\n",
       "      <td>Key Lime Pie</td>\n",
       "      <td>Sweet pie made with Key lime juice</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Disney's Halloween Festival</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Region          State         Attraction  \\\n",
       "0  Northeast       New York  Statue of Liberty   \n",
       "1  Northeast  Massachusetts      Freedom Trail   \n",
       "2    Midwest       Illinois       Willis Tower   \n",
       "3    Midwest       Michigan    The Great Lakes   \n",
       "4      South        Florida       Disney World   \n",
       "\n",
       "                              Description   Food Specialty  \\\n",
       "0                Iconic symbol of freedom           Bagels   \n",
       "1                 Historic 2.5-mile trail     Clam Chowder   \n",
       "2  One of the tallest buildings in the US  Deep-Dish Pizza   \n",
       "3       Largest group of freshwater lakes          Pasties   \n",
       "4            Famous theme park and resort     Key Lime Pie   \n",
       "\n",
       "                            Food Description Best Time to Visit  \\\n",
       "0               Boiled and baked dough rings         Year-round   \n",
       "1        Creamy soup with clams and potatoes               Fall   \n",
       "2  Thick crust pizza with layers of toppings             Summer   \n",
       "3     Pastry filled with meat and vegetables             Winter   \n",
       "4         Sweet pie made with Key lime juice             Summer   \n",
       "\n",
       "                   Notable Events  \n",
       "0      Fourth of July Celebration  \n",
       "1                 Boston Marathon  \n",
       "2                Taste of Chicago  \n",
       "3  Great Lakes Shipwreck Festival  \n",
       "4     Disney's Halloween Festival  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'us_tourism.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "dataframe_csv1 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "dataframe_csv1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5824bbd",
   "metadata": {},
   "source": [
    "## 1.2 Handle damaged csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88c9ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 8 fields in line 48, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m file_path2 = \u001b[33m'\u001b[39m\u001b[33mus_tourism2damage.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# # Display the first few rows of the DataFrame\u001b[39;00m\n\u001b[32m      8\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 8 fields in line 48, saw 9\n"
     ]
    }
   ],
   "source": [
    "file_path2 = 'us_tourism2damage.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(file_path2)\n",
    "\n",
    "# # Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e6af4",
   "metadata": {},
   "source": [
    "### Cannot read damage file by pandas\n",
    "\n",
    "We need to read file line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'us_tourism2damage.csv'\n",
    "\n",
    "#read line by line, and filter the bad data\n",
    "a = 0\n",
    "title = []\n",
    "gooddata = []\n",
    "baddata= []\n",
    "\n",
    "# Your code:\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if a == 0:\n",
    "            title = row\n",
    "        a += 1\n",
    "        if len(row) == 8 and a != 1:\n",
    "            gooddata.append(row)\n",
    "        else:\n",
    "            baddata.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fc8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gooddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61f65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gooddf = pd.DataFrame(gooddata,columns = title)\n",
    "gooddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff3487",
   "metadata": {},
   "source": [
    "## 2.1 Read json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab55147",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = glob.glob(\"*.json\")\n",
    "jsonfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the JSON file\n",
    "df = pd.read_json(jsonfiles[2],lines = True)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff3241",
   "metadata": {},
   "source": [
    "## 2.2 Handle damaged json Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the JSON file\n",
    "dfDamage = pd.read_json(jsonfiles[0],lines = True)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "dfDamage.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d97a6",
   "metadata": {},
   "source": [
    "### 2.2.1 Simple demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107af1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example JSON data as a list of strings (each string is a JSON object)\n",
    "json_data = [\n",
    "    '{\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"}',\n",
    "    '{\"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"}',\n",
    "    '{\"name\": \"Charlie\", \"age\": 35}',\n",
    "    '{\"name\": \"Bib\", \"age\": , \"city\": \"Seattle\"}',\n",
    "    '{\"name\": \"Bcob\", \"city\": \"Beijing\"}',\n",
    "    '{\"name\": \"Bob\", \"xxxx\": 30, \"city\": \"San Francisco\"}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for json_str in json_data:\n",
    "    try:\n",
    "        # Attempt to parse each line as JSON\n",
    "        json_obj = json.loads(json_str)\n",
    "\n",
    "        # Handle missing or extra data\n",
    "        if 'name' not in json_obj:\n",
    "            json_obj['name'] = None\n",
    "        if 'age' not in json_obj:\n",
    "            json_obj['age'] = None\n",
    "        if 'city' not in json_obj:\n",
    "            json_obj['city'] = None\n",
    "\n",
    "        # Append the valid JSON object to the list\n",
    "        data.append(json_obj)\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f'Invalid Json data')\n",
    "\n",
    "# data\n",
    "# Convert the list of dictionaries to a pandas DataFrame (optional)\n",
    "dfjson= pd.DataFrame(data)\n",
    "\n",
    "#print\n",
    "dfjson[['name','age','city']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89447692",
   "metadata": {},
   "source": [
    "### 2.2.3 Handle the damaged json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa0bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = 'damaged.json'  # Replace with your file path\n",
    "data = []\n",
    "title = ['Region', 'State', 'Attraction', 'Description', \n",
    " 'Food Specialty', 'Food Description', 'Best Time to Visit', 'Notable Events']\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            # Handle missing data by setting a default value\n",
    "            # Your code\n",
    "            for key in title:\n",
    "                if key not in json_obj:\n",
    "                    json_obj[key] = None\n",
    "                \n",
    "            \n",
    "            # Append the processed JSON object to the list\n",
    "            data.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Invalid Json data: {line}')\n",
    "            \n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame (optional)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e322695",
   "metadata": {},
   "source": [
    "## 3.1 Read Txt data\n",
    "\n",
    "Examples for **\"split\"**\n",
    "```python\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 'Region: Northeast State: New York'\n",
    "result_h = h.split(\" \")\n",
    "result_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de030f89-03d9-4409-a1fa-a51690c6984f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = 'unstructureddata.txt'  # use your file path\n",
    "\n",
    "# Function to process a block of text and return a dictionary\n",
    "def process_block(block):\n",
    "    entity = {}\n",
    "    for item in block: \n",
    "        key, value = item.split(\":\")\n",
    "        entity[key] = value\n",
    "    return entity\n",
    "\n",
    "# Read and process the file\n",
    "current_data= []\n",
    "result= []\n",
    "with open(file_path,'r') as file:\n",
    "    for line in file:\n",
    "        # Your code\n",
    "        if line.strip() == \"\":\n",
    "            dfcontent = process_block(current_data)\n",
    "            result.append(dfcontent)\n",
    "            current_data = []\n",
    "        else:\n",
    "            current_data.append(line.strip())\n",
    "            \n",
    "print(result[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c001d-baaf-416e-9417-a0be1a7d20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(result)\n",
    "# # Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3833f14",
   "metadata": {},
   "source": [
    "## 3.2 Handle damaged txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b090bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Your turn -- Class practice\"\"\"\n",
    "file_path = 'damaged_data.txt'  # use your file path\n",
    "\n",
    "# Define the expected keys\n",
    "expected_keys = [\"Region\", \"State\", \"Attraction\", \"Description\", \n",
    "                 \"Food Specialty\", \"Food Description\", \n",
    "                 \"Best Time to Visit\", \"Notable Events\"]\n",
    "\n",
    "\n",
    "# Function to process a block of text and return a dictionary\n",
    "def process_block_with_damaged(block):\n",
    "    entity = {key: None for key in expected_keys}\n",
    "    #Your code:\n",
    "    for item in block:\n",
    "        if \": \" in item:\n",
    "            key, value = item.split(\": \")\n",
    "            if key in entity:\n",
    "                entity[key] = value\n",
    "    \n",
    "    return entity\n",
    "\n",
    "# Read and process the file\n",
    "current_data= []\n",
    "result= []\n",
    "with open(file_path,'r') as file:\n",
    "    for line in file:\n",
    "        # Your code:\n",
    "        if line.strip() == \"\":\n",
    "            processed_data = process_block_with_damaged(current_data)\n",
    "            result.append(processed_data)\n",
    "            current_data = []\n",
    "        else:\n",
    "            current_data.append(line.strip())\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "dfnew = pd.DataFrame(result)\n",
    "\n",
    "# Display the DataFrame\n",
    "dfnew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dc81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38f27c02-ecb9-4c4e-b92e-7688f81e03e9",
   "metadata": {},
   "source": [
    "### Practice 🧪 Lab: Cleaning “Damaged” Data with pandas\n",
    "\n",
    "In this practice, you’ll practice detecting and fixing common data problems:\n",
    "\n",
    "- Rows with **missing fields**\n",
    "- Rows with **extra fields** (e.g., too many columns due to extra commas)\n",
    "- **Wrong types** (strings in numeric columns)\n",
    "- **Out-of-range values** (e.g., impossible GPA)\n",
    "- Empty strings vs true missing values (`NaN`)\n",
    "\n",
    "You will:  \n",
    "1) Load a deliberately “damaged” CSV,  \n",
    "2) Identify & filter bad rows,  \n",
    "3) Coerce types, and  \n",
    "4) Produce a clean DataFrame ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49362fd-70c9-4189-baf3-a3c80c3556c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 Create a deliberately “damaged” CSV (run this cell once)\n",
    "source_csv = \"\"\"Student_ID,Name,Major,Credits,GPA,Grad_Year\n",
    "1001,Alice,Computer Science,120,3.85,2026\n",
    "1002,Bob,Math, ,3.10,2025\n",
    "1003,Charlie,Physics,98,3O,2024\n",
    "1004,Diana,Economics,140,4.7,2023\n",
    "1005,Ed,Computer Science,129,N/A,2027\n",
    "1006, ,Biology,88,2.95,2028\n",
    "1007,Grace,Physics,101,3.40,2029,EXTRA_TOKEN\n",
    ",Henry,Math,64,2.70,2026\n",
    "1009,Ivy,,110,3.20,2025\n",
    "1010,Jack,Economics,215,3.50,2035\n",
    "1011,Kate,Chemistry,95,3.00,\n",
    "1012,Liam,Computer Science,NaN,3.60,2024\n",
    "1013,Mia,Math,77,,2025\n",
    "1014,Noah,Physics,85,3.25,2027\n",
    "1015,Olivia,Economics,,2.80,2026\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752a9f9-7167-4a9f-aceb-7d1022684c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
