{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edcae8d",
   "metadata": {},
   "source": [
    "# Evaluating a classification model\n",
    "In this lecture, we will demonstrate how to evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21260f46",
   "metadata": {},
   "source": [
    "## 1. Confusion matrix\n",
    "\n",
    "Table that describes the performance of a classification model\n",
    "\n",
    "## Agenda\n",
    "\n",
    "- What is the purpose of **model evaluation**, and what are some common evaluation procedures?\n",
    "- What is the usage of **classification accuracy**, and what are its limitations?\n",
    "- How does a **confusion matrix** describe the performance of a classifier?\n",
    "- What **metrics** can be computed from a confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert the feature matrix into a DataFrame\n",
    "iris_df = #your code \n",
    "\n",
    "# Convert string labels to numerical values\n",
    "label_encoder = #your code \n",
    "y_encoded = #your code \n",
    "\n",
    "# Add target variable (iris species) to the DataFrame\n",
    "iris_df['species'] = y_encoded\n",
    "\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct our Naive Bayes model\n",
    "X_train, X_test, y_train, y_test = #your code \n",
    "\n",
    "model = #your code \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for the testing set\n",
    "y_pred_class = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a35cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18e245-0893-4c36-96cd-8ae996fdf23a",
   "metadata": {},
   "source": [
    "### Let's build the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = #your code \n",
    "#your code \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d0ff2",
   "metadata": {},
   "source": [
    "##  Only use virginica and versicolor as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fd8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 25 true and predicted responses\n",
    "print('True:', y_test[0:25])\n",
    "print('Pred:', y_pred_class[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_class, labels=[1, 2]) \n",
    "#your code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc6441",
   "metadata": {},
   "source": [
    "**Basic terminology**\n",
    "\n",
    "- **True Positives (TP):** we *correctly* predicted that they *do* have diabetes\n",
    "- **True Negatives (TN):** we *correctly* predicted that they *don't* have diabetes\n",
    "- **False Positives (FP):** we *incorrectly* predicted that they *do* have diabetes (a \"Type I error\")\n",
    "- **False Negatives (FN):** we *incorrectly* predicted that they *don't* have diabetes (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfe13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = cm\n",
    "\n",
    "TP = cm[1, 1]  # True Positive: Virginica correctly identified as Virginica\n",
    "TN = cm[0, 0]  # True Negative: Versicolor correctly identified as Versicolor\n",
    "FP = cm[0, 1]  # False Positive: Versicolor incorrectly identified as Virginica\n",
    "FN = cm[1, 0]  # False Negative: Virginica incorrectly identified as Versicolor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a20bcd",
   "metadata": {},
   "source": [
    "### Metrics computed from a confusion matrix\n",
    "**Classification Accuracy:** Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70387597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((TP + TN) / (TP + TN + FP + FN))\n",
    "indices = #your code \n",
    "y_test_filtered = #your code \n",
    "y_pred_filtered = #your code \n",
    "\n",
    "# Now calculate the accuracy score\n",
    "accuracy = #your code \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c285fb9",
   "metadata": {},
   "source": [
    "**Classification Error:** Overall, how often is the classifier incorrect?\n",
    "\n",
    "- Also known as \"Misclassification Rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((FP + FN) / (TP + TN + FP + FN))\n",
    "print(1 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9066",
   "metadata": {},
   "source": [
    "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
    "\n",
    "- How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN / (TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bcf90",
   "metadata": {},
   "source": [
    "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731625d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FP / (TN + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf679136-f896-4214-a93e-dd8956cb6365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbd9c0e-db49-49f3-95e8-7c9bc9f1c41c",
   "metadata": {},
   "source": [
    "### Understanding Precision and Recall\n",
    "\n",
    "When evaluating a classifier, especially in data mining and machine learning, we often care about **how accurate** and **how complete** our predictions are.  \n",
    "Thatâ€™s where **Precision** and **Recall** come in.\n",
    "\n",
    "\n",
    "### Precision\n",
    "**Precision** answers the question:\n",
    "\n",
    "> \"Out of all the samples the model predicted as positive, how many were actually positive?\"\n",
    "\n",
    "It focuses on **prediction quality** â€” how *precise* the model is when it predicts a positive class.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **TP (True Positive):** Correctly predicted positive samples  \n",
    "- **FP (False Positive):** Incorrectly predicted positive samples (false alarms)\n",
    "\n",
    "*Example:*  \n",
    "If a model predicts 100 aircraft as \"Commercial\" and 90 are actually Commercial,  \n",
    "then Precision = 90 / 100 = **0.9 (90%)**\n",
    "\n",
    "\n",
    "### ðŸ”¹ Recall\n",
    "**Recall** (also known as **Sensitivity** or **True Positive Rate**) answers the question:\n",
    "\n",
    "> \"Out of all the actual positive samples, how many did the model correctly identify?\"\n",
    "\n",
    "It focuses on **completeness** â€” how *many* real positives the model managed to catch.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **TP (True Positive):** Correctly predicted positive samples  \n",
    "- **FN (False Negative):** Missed positive samples\n",
    "\n",
    " *Example:*  \n",
    "If there are 100 actual \"Commercial\" aircraft and the model correctly finds 90 of them,  \n",
    "then Recall = 90 / 100 = **0.9 (90%)**\n",
    "\n",
    "\n",
    "###  Precision vs. Recall Tradeoff\n",
    "There is often a **tradeoff** between Precision and Recall:\n",
    "- A model with **high Precision** makes fewer false alarms but may miss some positives.\n",
    "- A model with **high Recall** catches most positives but may produce more false alarms.\n",
    "\n",
    "Choosing which one to prioritize depends on your application:\n",
    "- In **fraud detection** or **medical diagnosis**, Recall is crucial.  \n",
    "- In **spam detection** or **recommendation systems**, Precision might matter more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d265cc-18d1-4d57-a776-4e4ebe0ce2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix and components\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"\\nTP={TP}, TN={TN}, FP={FP}, FN={FN}\")\n",
    "\n",
    "# Compute Precision and Recall\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba20f1-fa0d-4c25-8f38-e821e22326d7",
   "metadata": {},
   "source": [
    "### Sklearn Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a9830-323f-4568-acf3-72fc35236568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "precision = precision_score(y_test_filtered, y_pred_filtered)\n",
    "recall = recall_score(y_test_filtered, y_pred_filtered)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e52972-5ddd-41c5-b7e5-699360d7437e",
   "metadata": {},
   "source": [
    "# F1-Score: Balancing Precision and Recall\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sometimes, Precision and Recall give **conflicting results** â€” one may be high while the other is low.  \n",
    "To combine both into a single metric, we use the **F1-score**.\n",
    "\n",
    "The **F1-score** is the **harmonic mean** of Precision and Recall,  \n",
    "giving equal importance to both.\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 \\times \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n",
    "$$\n",
    "\n",
    "### Why Harmonic Mean?\n",
    "\n",
    "The harmonic mean punishes extreme values more than the arithmetic mean.  \n",
    "If either Precision or Recall is very low, the F1-score will also be low.  \n",
    "\n",
    "This makes it ideal for **imbalanced datasets** â€”  \n",
    "for example, detecting rare aircraft failures or rare fraudulent transactions.\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "| Metric | Value |\n",
    "|:-------|------:|\n",
    "| Precision | 0.9 |\n",
    "| Recall | 0.6 |\n",
    "| F1-score | \\( 2 Ã— (0.9 Ã— 0.6) / (0.9 + 0.6) = 0.72 \\) |\n",
    "\n",
    "The F1-score = **0.72**, showing a balance between the two metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0f573-e5b0-4339-90cd-4e4947f19795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1-score manually\n",
    "f1_score_manual = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score (manual): {f1_score_manual:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69583dc-392b-47bc-87f8-04d388c7e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# F1-score using sklearn\n",
    "f1 = f1_score(y_test_filtered, y_pred_filtered, pos_label=1)\n",
    "\n",
    "print(f\"F1-score (sklearn): {f1:.2f}\")\n",
    "\n",
    "# Full report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_filtered, y_pred_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e4645",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation\n",
    "\n",
    "## Model evaluation procedures\n",
    "\n",
    "1. **Training and testing on the same data**\n",
    "    - Rewards overly complex models that \"overfit\" the training data and won't necessarily generalize\n",
    "2. **Train/test split**\n",
    "    - Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
    "    - Better estimate of out-of-sample performance, but still a \"high variance\" estimate\n",
    "    - Useful due to its speed, simplicity, and flexibility\n",
    "3. **K-fold cross-validation**\n",
    "    - Systematically create \"K\" train/test splits and average the results together\n",
    "    - Even better estimate of out-of-sample performance\n",
    "    - Runs \"K\" times slower than train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70baf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e335f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = #your code \n",
    "\n",
    "# check classification accuracy of KNN with K=5\n",
    "knn = #your code \n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = #your code \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7c0f8",
   "metadata": {},
   "source": [
    "### Steps for K-fold cross-validation\n",
    "1. Split the dataset into K **equal** partitions (or \"folds\").\n",
    "2. Use fold 1 as the **testing set** and the union of the other folds as the **training set**.\n",
    "3. Calculate **testing accuracy**.\n",
    "4. Repeat steps 2 and 3 K times, using a **different fold** as the testing set each time.\n",
    "5. Use the **average testing accuracy** as the estimate of out-of-sample accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate splitting a dataset of 25 observations into 5 folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False).split(range(25))\n",
    "\n",
    "# print the contents of each training and testing set\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a6817",
   "metadata": {},
   "source": [
    "- Dataset contains **25 observations** (numbered 0 through 24)\n",
    "- 5-fold cross-validation, thus it runs for **5 iterations**\n",
    "- For each iteration, every observation is either in the training set or the testing set, **but not both**\n",
    "- Every observation is in the testing set **exactly once**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4148f",
   "metadata": {},
   "source": [
    "## Comparing cross-validation to train/test split\n",
    "Advantages of **cross-validation:**\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "Advantages of **train/test split:**\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "- Simpler to examine the detailed results of the testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900b757",
   "metadata": {},
   "source": [
    "### Cross-validation recommendations\n",
    "1. K can be any number, but **K=10** is generally recommended\n",
    "2. For classification problems, **stratified sampling** is recommended for creating the folds\n",
    "    - Each response class should be represented with equal proportions in each of the K folds\n",
    "    - scikit-learn's `cross_val_score` function does this by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25723f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "k_range = list(range(1, 31))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a50dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "#Please add your name on the title\n",
    "#your code\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17537d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac7901-7a92-466d-aa6a-848ef7ae1776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd7f500-bfb8-4d98-b2b0-f5e7e63006ba",
   "metadata": {},
   "source": [
    "# Confusion Matrix Practice â€“ From Scratch vs. Scikit-learn\n",
    "\n",
    "### Objective\n",
    "In this exercise, you will understand how a **confusion matrix** works and how to calculate the key evaluation metrics **manually** and using **scikit-learn**.\n",
    "\n",
    "You will:\n",
    "- Compute **Accuracy**, **Precision**, **Recall**, and **F1-Score** from scratch.  \n",
    "- Verify your results using built-in `scikit-learn` functions.  \n",
    "- Visualize the confusion matrix with Seaborn.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The following data represent predictions from a model that classifies aircraft as **Commercial (1)** or **Private (0)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fc110-15d9-4f4f-8368-7267cc6c2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# True labels (1 = Commercial, 0 = Private)\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "# Predicted labels\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef93f9-50d2-47a5-a782-4826dc28fd64",
   "metadata": {},
   "source": [
    "## Part 1 â€” Compute Manually (From Scratch)\n",
    "\n",
    "**Instructions:**\n",
    "1. Write Python code to manually calculate:\n",
    "   - True Positives (TP)\n",
    "   - True Negatives (TN)\n",
    "   - False Positives (FP)\n",
    "   - False Negatives (FN)\n",
    "2. Compute the following metrics:\n",
    "   - Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "   - Precision = TP / (TP + FP)\n",
    "   - Recall = TP / (TP + FN)\n",
    "   - F1 Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "3. Display the confusion matrix clearly as a 2Ã—2 structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352d4ec-0dba-415b-b01c-c353d77e2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069191-0909-42d5-9aa9-d28f8c493e05",
   "metadata": {},
   "source": [
    "## Part 2 â€” Verify with Scikit-learn\n",
    "\n",
    "**Instructions:**\n",
    "1. Use the scikit-learn functions below to confirm your manual results.\n",
    "2. Then visualize your confusion matrix using Seaborn.\n",
    "\n",
    "**Functions to use:**\n",
    "- `confusion_matrix()`\n",
    "- `accuracy_score()`\n",
    "- `precision_score()`\n",
    "- `recall_score()`\n",
    "- `f1_score()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd90b3-80ed-469c-aa1d-c09dabb276c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
